{
    "contents" : "## dataset: the input dataset which need analysing\n## ccol: the features of dataset which will be considered as dimensions\n## cluster: maximum clusters will be compared(from 2 to 19)\n## iter: iterated times of k-means and fcm algorithm\n## dist: the method will be used for fcm and pam, choice: manhattan and euclidean\n\n# cclass <- function(x, ...) UseMethod(\"cclass\")\n\ncclass= function (x, ccol = c(1:dim(x)[2]),cluster=19, iter = 100, dist=\"euclidean\")\n{\n  \n  dataset <- as.matrix(x[,ccol])\n\n  if(length(dataset) > length(na.omit(dataset)))\n  {\n    dataset <- na.omit(dataset)\n    warning(\"There are some missing data in your choosing range, they have been removed from matrix\")\n  }\n  \n  isNum <- sapply(dataset, is.numeric)\n  for(i in length(isNum))\n    if(isNum[i] == FALSE)\n    {\n      stop(\"Sorry, the data you chosen must be numeric.\")\n    }\n  \n  \n  wide     = dim(dataset)[2]\n  length   = dim(dataset)[1]\n  \n  ### load several libraries (some of them maybe are not needed, but I don't know anymore which one...):\n  library(cclust)\n  library(e1071)\n  library(cluster)\n  \n  ### plot function has been defined here\n  ### which will return the plot it made\n  \n  bip = function (x, choices = 1:2, scale = 1, pc.biplot = FALSE, col=NULL, color,...) \n  {\n    if (length(choices) != 2) \n      stop(\"length of choices must be 2\")\n    if (!length(scores <- x$scores)) \n      stop(gettextf(\"object '%s' has no scores\", deparse(substitute(x))), \n           domain = NA)\n    lam <- x$sdev[choices]\n    if (is.null(n <- x$n.obs)) \n      n <- 1\n    lam <- lam * sqrt(n)\n    if (scale < 0 || scale > 1) \n      warning(\"'scale' is outside [0, 1]\")\n    if (scale != 0) \n      lam <- lam^scale\n    else lam <- 1\n    if (pc.biplot) \n      lam <- lam/sqrt(n)\n    bips(t(t(scores[, choices])/lam), t(t(x$loadings[, choices]) * lam), col = color, ...)\n    legend(\"bottomright\", c(color$legendName),\n           pch=c(color$legendPch), col=c(color$legendColor), bty=\"o\",cex = 1,y.intersp = 0.8)\n    x <- recordPlot()\n    return(x)\n  }\n    \n  bips = function (x, y, var.axes = TRUE, col, col2=\"black\", cex = rep(par(\"cex\"), 2), \n                   xlabs = NULL, ylabs = NULL, expand = 1, xlim = NULL, ylim = NULL, \n                   arrow.len = 0.1, main = NULL, sub = NULL, xlab = NULL, ylab = NULL, \n                   ...) \n  {\n    n <- nrow(x)\n    p <- nrow(y)\n    if (missing(xlabs)) {\n      xlabs <- dimnames(x)[[1]]\n      if (is.null(xlabs)) \n        xlabs <- 1:n\n    }\n    xlabs <- as.character(xlabs)\n    dimnames(x) <- list(xlabs, dimnames(x)[[2]])\n    if (missing(ylabs)) {\n      ylabs <- dimnames(y)[[1]]\n      if (is.null(ylabs)) \n        ylabs <- paste(\"Var\", 1:p)\n    }\n    if (length(cex) == 1) \n      cex <- c(cex, cex)\n    unsigned.range <- function(x) c(-abs(min(x)), abs(max(x)))\n    rangx1 <- unsigned.range(x[, 1])\n    rangx2 <- unsigned.range(x[, 2])\n    rangy1 <- unsigned.range(y[, 1])\n    rangy2 <- unsigned.range(y[, 2])\n    if (missing(xlim) && missing(ylim)) \n      xlim <- ylim <- rangx1 <- rangx2 <- range(rangx1, rangx2)\n    else if (missing(xlim)) \n      xlim <- rangx1\n    else if (missing(ylim)) \n      ylim <- rangx2\n    ratio <- max(rangy1/rangx1, rangy2/rangx2)/expand\n    on.exit(par(op))\n    op <- par(pty = \"s\")\n    if (!is.null(main)) \n      op <- c(op, par(mar = par(\"mar\") + c(0, 0, 1, 0)))\n    plot(x, type = \"n\", xlim = xlim, ylim = ylim, col = col$colors,   \n         xlab = xlab, ylab = ylab, sub = sub, main = main,...)\n    \n    for(i in 1 : n)\n    {\n      if(col$colors[i] == col$legendColor[1])\n      {\n        points(x[i,1],x[i,2],cex = cex[1], col = col$colors[i],pch = 4,...)\n      }\n      else \n      {\n        points(x[i,1],x[i,2],cex = cex[1], col = col$colors[i],pch = 1,...)\n      }\n    }\n    par(new = TRUE)\n    plot(y, axes = FALSE, type = \"n\", xlim = xlim * ratio, ylim = ylim * \n           ratio, xlab = \"\", ylab = \"\", col = col2, ...)\n    box(col = col2)\n    text(y, labels = ylabs, cex = cex[2], col = col2, ...)   ### general use\n    if (var.axes)                                            ### general use \n      arrows(0, 0, y[,1] * 0.8, y[,2] * 0.8, col = \"black\",   ### general use\n             length = arrow.len)  ### general use\n  }\n  \n  ## function to align the labels\n  align <- function(final,label)\n  {\n    \n    #function to pick out rows\n    pick<-function(data,row,value,choose){\n      length = dim(data)[1]\n      row_length = table(data[,row] == value)[2]\n      col_length = length(choose)\n      result = matrix(nrow = row_length, ncol = col_length)\n      \n      a = 1\n      b = 1\n      \n      for(i in 1:length){\n        if (data[b,row] == value ){\n          result[a,] = data[b,choose]\n          a = a+1\n        } \n        b = b+1\n      }\n      \n      head(result)\n      return (result) \n    }\n    \n    #function to compare distant\n    dis<-function(a,b){\n      sqrt(sum((a-b)^2))\n    }\n  \n    length = dim(final)[1]\n    wide   = dim(final)[2]\n    length_label = length(label)  \n    l = length(table(final[,label[1]])) \n    length_attr = wide - length_label \n    attr = 1:length_attr  \n    \n    \n    pivot = array(0,c(l,length_attr,length_label))\n    \n    \n    #calculate the centers of the first label attribute\n    for(w in 1:length_label){ \n      for(u in 1:l){\n        pivot[u,,w] <-colMeans(pick(final,label[w],u,attr))\n      }\n    }\n    \n    #initialize the rank array\n    coff = array(0,c(l,l,length_label-1))\n    \n    \n\n#  get the rank table based on the distance matrix\nfor(a in 1:(length_label-1)){ \n  for(b in 1:l){\n    for(c in 1:l){ \n      distant = dis(pivot[c,,1],pivot[b,,a+1])\n      coff[c,b,a] = distant\n    }\n  }\n}\n\n#   the distance matrix\n    per = permutations(n = l)\n    sum_row = nrow(per)\n    sum = rep(data = 0,sum_row)\n    rank = matrix(data = NA, nrow = l, ncol = l)\n\n    for(i in 1:2)\n    {\n      for(j in 1:sum_row)\n      {\n        for(a in 1:(length_label-1)) \n        {\n          for(x in 1:(length_label-1))\n          {\n            sum[j] = sum[j] + coff[x,per[a,x],i]\n          }\n        }\n      }\n      rank[,i] <- t(per[which.min(sum),])    \n    }\n  \n\n    \n    #according to the rank table, assign the new value to the orginal data \n    for(ccc in 1:length) {  \n      for(aaa in 1:(length_label-1)){  \n        final[ccc,label[1]+aaa] = rank[final[ccc,label[1]+aaa],aaa]\n      }  \n    }\n    \n    return(final)\n    \n  }\n\n  ## function to select best number of clusters\n  cluster_num =function (dataset,cluster)\n  {\n    ### apply K-means from 2 to 20 clusters and build matrix of validity indeces   \n    cluster_table1 <- data.frame(index=c('Cal','Scott','Marr','TracW','Fried'))\n    cluster_table2 <- data.frame(index=c('Fuz','PartDen','Xie','PartCoe'))\n    isCluster <- TRUE\n    ## Index function for pam and k-means\n    plot_indices <- function(indexes,cluster)\n    {\n      diff_cal <- rep(NA,cluster)\n      diff_scott <- rep(NA,cluster)\n      diff_marr <- rep(NA,cluster)\n      diff_tracw <- rep(NA,cluster)\n      diff_fried <- rep(NA,cluster)\n      output <- list()\n      \n      for (j in 2:cluster) {\n        diff_cal[j] <- (indexes[j+1,1]-indexes[j,1])-(indexes[j,1]-indexes[j-1,1])}\n      for (j in 2:cluster) {\n        diff_scott[j] <- (indexes[j,6]-indexes[j-1,6])}\n      for (j in 2:cluster) {\n        diff_marr[j] <- (indexes[j+1,7]-indexes[j,7])-(indexes[j,7]-indexes[j-1,7])}\n      for (j in 2:cluster) {\n        diff_tracw[j] <- (indexes[j+1,10]-indexes[j,10])-(indexes[j,10]-indexes[j-1,10])}\n      for (j in 2:cluster) {\n        diff_fried[j] <- (indexes[j,11]-indexes[j-1,11])}\n      output$bestCluster=c(which.min(diff_cal),which.max(diff_scott),\n                           which.max(diff_marr),which.max(diff_tracw),which.max(diff_fried))\n      output$clusterRank <- matrix(data = 0, nrow = 5, ncol = cluster - 1)\n      for(i in 2:cluster)\n      {\n        output$clusterRank[1,i-1] <- rank(diff_cal)[i]\n        output$clusterRank[2,i-1] <- cluster-rank(diff_scott)[i]\n        output$clusterRank[3,i-1] <- cluster-rank(diff_marr)[i]\n        output$clusterRank[4,i-1] <- cluster-rank(diff_tracw)[i]\n        output$clusterRank[5,i-1] <- cluster-rank(diff_fried)[i]\n      }\n      return(output)\n    }\n    \n    ## Patch the indexes for one cluster\n    addClusterOne <- function(indexes,inputDataset)\n    {\n      \n      ## Calculate scott index, marriot, tracew and friedman index when cluster is 1.\n      rows <- nrow(inputDataset)\n      oneCluster <- rep(as.integer(1),time=rows)\n      \n      \n      ttww <- function(x, clsize, cluster) {\n        n <- sum(clsize)\n        k <- length(clsize)\n        w <- 0\n        tt <- cov(x) * n\n        for (l in 1:k) w <- w + cov(x[cluster == l, ]) * clsize[l]\n        zttw <- list(tt = tt, w = w)\n        return(zttw)\n      }\n      \n      scott <- function(zttw, clsize) {\n        n <- sum(clsize)\n        dettt <- prod(eigen(zttw$tt)$values)\n        detw <- prod(eigen(zttw$w)$values)\n        scott <- n * log(dettt/detw)\n        return(scott)\n      }\n      marriot <- function(zttw, clsize) {\n        k <- length(clsize)\n        detw <- prod(eigen(zttw$w)$values)\n        mar <- (k^2) * detw\n        return(mar)\n      }\n      tracew <- function(zttw) {\n        tracew <- sum(diag(zttw$w))\n        return(tracew)\n      }\n      friedman <- function(zttw) {\n        b <- zttw$tt - zttw$w\n        fried <- sum(diag(solve(zttw$w) %*% b))\n        return(fried)\n      }\n      zttw<-ttww(inputDataset,rows,oneCluster)\n      zscott<-scott(zttw,rows)\n      zmarriot<-marriot(zttw,rows)\n      ztracew<-tracew(zttw)\n      zfriedman<-friedman(zttw)\n      \n      ## Calinski\n      indexes[1,1] <- 0\n      ## scott\n      indexes[1,6] <- zscott\n      ## Marriot\n      indexes[1,7] <- zmarriot\n      ## TraceW\n      indexes[1,10] <- ztracew\n      ## Fried\n      indexes[1,11] <- zfriedman\n      \n      return (indexes)\n    }\n    \n## function to plot the fuzzy val indices\n    plot_indices_fcm <- function(indexes_fcm,cluster)\n    {\n      diff_fhv <- indexes_fcm[,1]\n      diff_pd <- indexes_fcm[,3]\n      diff_xb <- indexes_fcm[,4]\n      diff_pc <- indexes_fcm[,6]\n      \n      output <- list() \n      output$bestCluster=c(which.min(diff_fhv),which.max(diff_pd),which.min(diff_xb),\n                           which.max(diff_pc))\n      output$clusterRank <- matrix(data = 0, nrow = 4, ncol = cluster - 1)\n      for(i in 2:cluster)\n      {\n        output$clusterRank[1,i-1] <- rank(diff_fhv)[i]\n        output$clusterRank[2,i-1] <- cluster-rank(diff_pd)[i]\n        output$clusterRank[3,i-1] <- rank(diff_xb)[i]\n        output$clusterRank[4,i-1] <- cluster-rank(diff_pc)[i]\n      }\n      return(output)\n    }\n    \n## Calculate best Cluster number\n    bestCluster <- function(kmRankTable, pamRankTable, fcmRankTable, cluster)\n    {\n      finalRankTable <- rep(NA, cluster - 1)\n      a <- 0\n      b <- 0\n      c <- 0\n      for(i in 1:(cluster - 1))\n      {\n        for(l in 1:5)\n        {\n          c <- c + kmRankTable[l,i] * (3/8)\n        }\n        for(j in 1:5)\n        {\n          a <- a + pamRankTable[j,i] * (3/8)\n        }\n        for(k in 1:4)\n        {\n          b <- b + fcmRankTable[k,i] * (1/4)\n        }\n        finalRankTable[i] <- a + b + c\n        a <- 0\n        b <- 0\n        c <- 0\n      }\n      return(which.min(finalRankTable)+1)\n    }\n    \n    while(isCluster){\n    isCluster <- FALSE\n    kmIndexes <- matrix(data = 0, nrow = cluster + 1, ncol = 15)\n    pamIndexes <- matrix(data = 0, nrow = cluster + 1, ncol = 15)\n    fcmIndexes <- matrix(data = NA, nrow = cluster, ncol = 9)\n    kmRankTable <- matrix(data = 0, nrow = 5, ncol = (cluster - 1))\n    pamRankTable <- matrix(data = 0, nrow = 5, ncol = (cluster -1))\n    fcmRankTable <- matrix(data = 0, nrow = 4, ncol = (cluster - 1))\n    \n    ### HCA algorithm\n    h <- hclust(dist(dataset), method=\"average\")\n    \n    ### K-means part\n    for (i in 2:(cluster+1)) {\n      initial <- tapply(dataset, list(rep(cutree(h,i), ncol(dataset)), col(dataset)), mean)\n      cl <- kmeans(dataset,initial,iter)\n      if(suppressWarnings(min(cl$size) == 1))\n      {\n        cluster <- i - 2\n        isCluster <- TRUE\n        warning(paste(\"The cluster of current dataset cannot be set more than\",cluster))\n        break\n      }\n      \n      if(\"try-error\" %in% class(try(suppressWarnings(clustIndex(cl,dataset,index=\"all\")))))\n      {\n        cluster <- i - 2\n        if(cluster < 2)\n        {\n          stop(\"The indexes cannot be calculated\")\n        }\n        isCluster <- TRUE\n        warning(paste(\"The cluster of current dataset cannot be set more than\",cluster))\n        break\n      }\n      kmIndexes[i,] <- suppressWarnings(clustIndex(cl,dataset,index=\"all\"))\n    }\n    \n    ## If cluster is reset, start another circle of this loop    \n    if(isCluster)\n    {\n      next\n    }\n \n    kmIndexes <- addClusterOne(kmIndexes,dataset)   \n    KM<-list()\n    KM<-plot_indices(kmIndexes,cluster)\n    cluster_table1<-cbind(cluster_table1,KM$bestCluster)\n    kmRankTable <- KM$clusterRank\n    \n   miowss <- function(centri, cluster, dati) {\n      retval <- rep(0, nrow(centri))\n      x <- (dati - centri[cluster, ])^2\n      for (k in 1:nrow(centri)) {\n        retval[k] <- sum(x[cluster == k, ])\n      }\n      retval\n    }\n    \n    ## PAM algorithm\n    for (i in 2:(cluster + 1)) {\n      cl <- pam(dataset, i, diss = inherits(dataset, \"dist\"), metric = dist,\n                                  medoids = NULL, stand = FALSE, cluster.only = FALSE)\n      if(suppressWarnings(min(cl$size) == 1))\n      {\n        cluster <- i - 2\n        isCluster <- TRUE\n        warning(paste(\"The cluster of current dataset cannot be set more than\",cluster))\n        break\n      }\n      cl$centers <- cl$medoids\n      cl$cluster <- cl$clustering\n      cl$size <- table(cl$cluster)[]\n      cl$withins <- miowss(cl$centers, cl$cluster, dataset)\n      \n      if(\"try-error\" %in% class(try(suppressWarnings(clustIndex(cl, dataset, index=\"all\")))))\n      {\n        cluster <- i - 2\n        if(cluster < 2)\n        {\n        stop(\"The indexes cannot be calculated\")\n        }\n        isCluster <- TRUE\n        warning(paste(\"The cluster of current dataset cannot be set more than\",cluster))\n        break\n      }\n      pamIndexes[i,] <- suppressWarnings(clustIndex(cl, dataset, index=\"all\"))\n    }\n   \n   ## If cluster is reset, start another circle of this loop \n   if(isCluster)\n   {\n      next\n   } \n    pamIndexes <- addClusterOne(pamIndexes,dataset)\n    \n    PAM <- plot_indices(pamIndexes,cluster)\n    cluster_table1<-cbind(cluster_table1,PAM$bestCluster)\n    pamRankTable <- PAM$clusterRank\n    \n    for (i in 2:cluster) {\n      initial <- tapply(dataset, list(rep(cutree(h,i), ncol(dataset)), col(dataset)), mean)\n      cl <- cmeans(dataset,initial, iter,verbose=FALSE,dist = dist, method=\"cmeans\",m=2,initial)\n      if(suppressWarnings(min(cl$size) == 1 ))\n      {\n        cluster <- i - 2\n        isCluster <- TRUE\n        break\n      }\n      \n      \n      if(\"try-error\" %in% class(try(suppressWarnings(fclustIndex(cl,dataset,index=\"all\")))))\n      {\n        cluster <- i - 2\n        if(cluster < 2)\n        {\n          stop(\"The indexes cannot be calculated\")\n        }\n        isCluster <- TRUE\n        warning(paste(\"The cluster of current dataset cannot be set more than\",cluster))\n        break\n      }\n      fcmIndexes[i,] <- suppressWarnings(fclustIndex(cl,dataset,index=\"all\"))\n    }\n    \n   ## If cluster is reset, start another circle of this loop \n   if(isCluster)\n   {\n     next\n   }\n    FCM <- list()\n    FCM<-plot_indices_fcm(fcmIndexes,cluster)\n    cluster_table2<-cbind(cluster_table2,FCM$bestCluster)\n    fcmRankTable <- FCM$clusterRank\n   }\n    clusterNum <- 0\n    clusterNum <- bestCluster(kmRankTable, pamRankTable, fcmRankTable,cluster)    \n    return (clusterNum)\n  }\n\n\n  ## function to generate the representing color according to the number of cluster\n  cluster_color <- function(cluster, final_dataset)\n  {\n    color_cluster <- list()\n    dataset_number <- dim(final_dataset)[1]\n    colors_use <- rainbow(cluster + 1)\n    cluster_label = rep(NA, cluster + 1)\n    cluster_label[1] <- \"NC\"\n    colors_output <- rep(NA, dataset_number)\n    pch_sty <- rep(4, dataset_number)\n    for(i in 1 : cluster){\n      cluster_label[i+1] <- i \n    }\n    for(j in 1 : dataset_number)\n    {\n      for(k in 1 : (cluster + 1))\n      {\n        if(final_dataset$class[j] == cluster_label[k])\n        {\n          colors_output[j] <- colors_use[k]\n          break\n        }\n      }\n    }\n    for(l in 1 : (cluster + 1))\n    {\n      cluster_label[l] <- paste(\"Cluster\",cluster_label[l],sep=\" \")\n    }\n    for(m in 2 : (cluster + 1))\n    {\n      pch_sty[m] <- 1\n    }\n    color_cluster$colors <- colors_output\n    color_cluster$legendName <- cluster_label\n    color_cluster$legendColor <- colors_use\n    color_cluster$legendPch <- pch_sty\n    return(color_cluster)\n  }\n  \n  \n  \n  ## find out the number of labels (clusters)\n  number = cluster_num(dataset, cluster)\n  \n  #create the tree of the orginal tree\n  h <- hclust(dist(dataset), method=\"average\")\n  \n  ##Kmeans\n  initial <- tapply(dataset, list(rep(cutree(h,number), ncol(dataset)), col(dataset)), mean)\n  cl_km <- kmeans(dataset,initial, iter)\n  km <- data.frame(dataset, clust=cl_km$cluster)    \n  \n  ##pam\n  cl_pam <- pam(dataset, number, diss = inherits(dataset, \"dist\"), metric = dist,\n                 medoids = NULL, stand = FALSE, cluster.only = FALSE)\n  pam <- data.frame(dataset, clust=cl_pam$clustering)  \n  \n  ##FCM\n  initial <- tapply(dataset, list(rep(cutree(h,number), ncol(dataset)), col(dataset)), mean)\n  cl <- cmeans(dataset,initial,iter,verbose=FALSE,dist= dist, method=\"cmeans\",m=2,initial)\n  fcm <- data.frame(dataset, clust=cl$cluster)\n  \n  #construct all results together\n  classif <- data.frame(cbind(km$clust, pam$clust, fcm$clust))\n  names(classif) <- c(\"km\", \"pam\", \"fcm\")\n  result <- data.frame(dataset,classif)\n  result = as.matrix(result)\n  \n  #align label, according the number of clutering methods, the paramater can be changed\n  aligned_result = align(result,(wide+1):(wide+3))\n  \n  interss = c()\n  for(i in 1:length){\n    if(aligned_result[i,wide+1] == aligned_result[i,wide+2] & aligned_result[i,wide+2] == aligned_result[i,wide+3] ){\n      interss = c(interss,as.integer(aligned_result[i,wide+1]))\n    }else{\n      interss = c(interss,\"NC\")\n    }\n  }\n\n  common <- list()  \n  \ncommon$cluster <- data.frame(dataset, km = cl_km$cluster, pam = cl_pam$clustering, fcm = cl$cluster, class=interss) \n  \n  colors <- cluster_color(number, common$cluster)\n  common$plot <- bip(princomp(common$cluster[,1:wide], cor=T),color = colors)\n  \n  common$number = number\n  \n  class(common) <- \"cclass\"\n  common\n}\n\nprint.cclass <- function(x)\n{\n  wide     = dim(x$cluster)[2]\n  table    =  table((x$cluster)[,wide])\n  length_label = length(table)\n  \n  cat(\"The best number of clusters is \", x$number,\"\\n\\n\")\n  \n  for(i in 1:(length_label-1) ){\n    cat(\"The number of instances assigned to label\", i,\"is\",table[i],\"\\n\")\n  }\n    \n  cat(\"The number of instances assigned to NC is\",table[length_label],\"\\n\")\n}",
    "created" : 1407231397308.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "129|42|148|4|\n151|23|153|4|\n610|1|622|0|\n",
    "hash" : "1246413818",
    "id" : "56D816EF",
    "lastKnownWriteTime" : 1407716134,
    "path" : "~/cclass/R/main_function_02_8.s",
    "project_path" : "R/main_function_02_8.s",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}